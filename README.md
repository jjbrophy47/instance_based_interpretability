# Training Data Analysis Papers
Papers related to instance-based interpretability (e.g. influence functions, prototypes, etc.), training dynamics, memorization/forgetting, etc.

<!--# 2022-->

<!--Dang et al. [Groupâ€™s Influence Value in Logistic Regression Model and Gradient Boosting Model](https://link.springer.com/chapter/10.1007/978-981-16-2377-6_66). In ICICT-->

## 2021

Basu et al. [Influence Functions in Deep Learning Are Fragile](https://openreview.net/forum?id=xHKVVHGDOEk). In ICLR 2021.

Chen et al. [HyDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/16871). In AAAI 2021.

D'souza et al. [A Tale Of Two Long Tails](https://arxiv.org/abs/2107.13098). In UDL-ICML Workshop 2021.

Hanawa et al. [Evaluation of Similarity-based Explanations](https://arxiv.org/abs/2006.04528). in ICLR 2021.

Harutyunyan et al. [Estimating informativeness of samples with Smooth Unique Information](https://openreview.net/forum?id=kEnBH98BGs5). In ICLR 2021.

Kong and Chaudhuri. [Understanding Instance-based Interpretability of Variational Auto-Encoders](https://papers.nips.cc/paper/2021/hash/13d7dc096493e1f77fb4ccf3eaf79df1-Abstract.html). In NeurIPS 2021.

Paul et al. [Deep Learning on a Data Diet: Finding Important Examples Early in Training](https://proceedings.neurips.cc/paper/2021/hash/ac56f8fe9eea3e4a365f29f0f1957c55-Abstract.html). In NeurIPS 2021.

Sui et al. [Representer Point Selection via Local Jacobian Expansion for Post-hoc Classifier Explanation of Deep Neural Networks and Ensemble Models](https://proceedings.neurips.cc//paper/2021/hash/c460dc0f18fc309ac07306a4a55d2fd6-Abstract.html). In NeurIPS 2021.

Terashita et al. [Influence Estimation for Generative Adversarial Networks](https://openreview.net/forum?id=opHLcXxYTC_). In ICLR 2021.

Zhang et al. [On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation](https://aclanthology.org/2021.acl-long.419/). In ACL 2021.

## 2020

Barshan et al. [RelatIF: Identifying Explanatory Training Samples via Relative Influence](http://proceedings.mlr.press/v108/barshan20a.html). In AISTATS 2020.

Basu et al. [On Second-Order Group Influence Functions for Black-Box Predictions](http://proceedings.mlr.press/v119/basu20b.html). In ICML 2020.

Brophy and Lowd. [TREX: Tree-Ensemble Representer-Point Explanations](https://arxiv.org/abs/2009.05530). In XXAI-ICML Workshop 2020.

Chen et al. [Multi-Stage Influence Function](https://proceedings.neurips.cc/paper/2020/hash/95e62984b87e90645a5cf77037395959-Abstract.html). In NeurIPS 2020.

Feldman and Zhang. [What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation](https://openreview.net/forum?id=mfoH69cSCz8). In NeurIPS 2020.

Vitaly Feldman. [Does learning require memorization? a short tale about a long tail](https://dl.acm.org/doi/abs/10.1145/3357713.3384290). In STOC 2020.

Agarwal et al. [Estimating Example Difficulty Using Variance of Gradients](https://arxiv.org/abs/2008.11600). In WHI-ICML Workshop 2020.

Jacovi and Goldberg. [Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?](https://aclanthology.org/2020.acl-main.386/). In ACL 2020.

Pleiss et al. [Identifying Mislabeled Data using the Area Under the Margin Ranking](https://papers.nips.cc/paper/2020/hash/c6102b3727b2a7d8b1bb6981147081ef-Abstract.html). In NeurIPS 2020.

Pruthi et al. [Estimating Training Data Influence by Tracing Gradient Descent](https://proceedings.neurips.cc/paper/2020/hash/e6385d39ec9394f2f3a354d9d2b88eec-Abstract.html). In NeurIPS 2020.

Swayamdipta et al. [Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics](https://openreview.net/forum?id=gW72G4zSdR1). In EMNLP 2020.

Yoon et al. [Data Valuation using Reinforcement Learning](http://proceedings.mlr.press/v119/yoon20a.html). In ICML 2020.

## 2019

Brunet et al. [Understanding the Origins of Bias in Word Embeddings](http://proceedings.mlr.press/v97/brunet19a.html). In ICML 2019.

Charpiat et al. [Input Similarity from the Neural Network Perspective](https://proceedings.neurips.cc/paper/2019/hash/c61f571dbd2fb949d3fe5ae1608dd48b-Abstract.html). In NeurIPS 2019.

Chen et al. [This Looks Like That: Deep Learning for Interpretable Image Recognition](https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html). In NeurIPS 2019.

Ghorbani and Zou. [Data Shapley: Equitable Valuation of Data for Machine Learning](http://proceedings.mlr.press/v97/ghorbani19c.html). In ICML 2019.

Hara et al. [Data Cleansing for Models Trained with SGD](https://proceedings.neurips.cc/paper/2019/hash/5f14615696649541a025d3d0f8e0447f-Abstract.html). In NeurIPS 2019.

Jia et al. [Towards Efficient Data Valuation Based on the Shapley Value](http://proceedings.mlr.press/v89/jia19a.html). AISTATS 2019.

Khanna et al. [Interpreting Black Box Predictions using Fisher Kernels](http://proceedings.mlr.press/v89/khanna19a.html). In AISTATS 2019.

Koh et al. [On the Accuracy of Influence Functions for Measuring Group Effects](https://papers.nips.cc/paper/2019/hash/a78482ce76496fcf49085f2190e675b4-Abstract.html). In NeurIPS 2019.

## 2018

Scharchilev et al. [Finding Influential Training Samples for Gradient Boosted Decision Trees](http://proceedings.mlr.press/v80/sharchilev18a.html). In ICML 2018.

Toneva et al. [An Empirical Study of Example Forgetting during Deep Neural Network Learning](https://arxiv.org/abs/1812.05159). In ICLR 2018.

Yeh et al. [Representer Point Selection for Explaining Deep Neural Networks](https://proceedings.neurips.cc/paper/2018/hash/8a7129b8f3edd95b7d969dfc2c8e9d9d-Abstract.html). In NeurIPS 2018.

## 2017

Koh and Liang. [Understanding Black-box Predictions via Influence Functions](http://proceedings.mlr.press/v70/koh17a). In ICML 2017.

## Before 2017

Cook and Weisberg. [Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression
](https://www.tandfonline.com/doi/abs/10.1080/00401706.1980.10486199). In Technometrics 1980.

Kim et al. [Examples are not enough, learn to criticize! Criticism for Interpretability](https://proceedings.neurips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html). In NeurIPS 2016.
